{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paudelsushil/adleosus/blob/main/graded_assignment4_sus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font color='red'> Total grade assignment 4: 46.5/50 </font>**\n",
        "\n",
        "**<font color='blue'> Good job! </font>**"
      ],
      "metadata": {
        "id": "JEfKRA0tB6h2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM6FkL2SjM6t"
      },
      "source": [
        "# Assignment 4 - More Semantic Segmentation"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "GjNUJKp6_kGE"
      },
      "source": [
        "---\n",
        "toc: true\n",
        "toc-depth: 6\n",
        "number-sections: true\n",
        "number-depth: 6\n",
        "execute:\n",
        "  eval: false\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63sSTy-BjbS_"
      },
      "source": [
        "## Instructions\n",
        "\n",
        "Be aware that the code cell for `customDataset` in this template is with complete instruction.\n",
        "\n",
        "Please follow the [instructions](assignments-setup.qmd) for setting up, completing, and submitting your assignments.\n",
        "\n",
        "<font color='red'>\n",
        "\n",
        "**Important Note**:\n",
        "\n",
        "- If you are using ChatGPT or any other AI based services to correct your writings or errors in code, you are required to provide a citation link.\n",
        "- For answers to theoretical questions you should provide references for the source of your answers.\n",
        "\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU95NKzqm0Lh"
      },
      "source": [
        "## Theoretical questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRPY_FYQx7gY"
      },
      "source": [
        "\n",
        "\n",
        "### **Q1.**\n",
        "\n",
        "**a)**\n",
        "Can you explain the limitations or drawbacks associated with each of the following methods used to extend the receptive field for deep learning in semantic segmentation:\n",
        "\n",
        "- Using a larger kernel size for conv layers throughout the network.\n",
        "\n",
        "- Increasing the depth of the network by stacking more convolutional layers.\n",
        "\n",
        "- Downsampling using pooling or convolution with a stride greater than 1.\n",
        "\n",
        "- Using dilated convolutions.\n",
        "\n",
        "(5 points)\n",
        "\n",
        "**b)**\n",
        "What are the proposed solutions to the drawbacks of using dilated conv layers?\n",
        "\n",
        "(5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-OwnOYNmudn"
      },
      "source": [
        "### Answer to Q1\n",
        "\n",
        "a)\n",
        "1. The limitations on using a larger kernel size for convolutional layers throughout the network are:\n",
        "\n",
        "    - Increased computational complexity\n",
        "\n",
        "    - Loss of local information\n",
        "\n",
        "    - Difficulty in learning hierarchial features\n",
        "\n",
        "2. Limitations on Increasing the depth of the network by stacking more convolutional layers are:\n",
        "\n",
        "    - Vanishing or exploding gradients\n",
        "\n",
        "    - Overfitting\n",
        "\n",
        "    - Model complexity\n",
        "\n",
        "3. The limitations associated with Downsampling using pooling or convolution with a stride greater than 1 are:\n",
        "\n",
        "    - Loss of spatial information\n",
        "\n",
        "    - aliasing effects\n",
        "\n",
        "    - reduced localization accuracy\n",
        "\n",
        "4. The limitation associated with Using Dilated convolutions method are:\n",
        "\n",
        "    - Grid effect\n",
        "\n",
        "    - High Computational cost\n",
        "\n",
        "    - Memory consumption\n",
        "\n",
        " (sam et al., 2023 and https://chat.openai.com/)\n",
        "\n",
        "B)  The proposed solutions to the drawbacks of using dilated conv layers are:\n",
        "\n",
        "- Using different dialation rates: Different dialtion rates can be used to achieve different receptive field sizes.\n",
        "\n",
        "- Using different filter sizes: Different filter sizes can be used to capture different levels of detail.\n",
        "\n",
        "- Using different activation functions: Different activation functions can be used to improve the performance of the dilated convolution.\n",
        "\n",
        " (sam et al., 2023 and https://chat.openai.com/)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font color='red'> 3/5 points on Q1</font>**\n",
        "\n",
        "\n",
        "**<font color='blue'> check below: B)  A proposed solution to the gridding artifacts of using dilated convolutional layers is called Efficient Spatial Pyramiding (ESP) block. An ESP block arranges the layers in parallel branches and has feature maps of earlier layers that have lower dilation rates as the adjacent branch of lower layers with higher dilation rates (p.33-34). Another solution is Hybrid Dilated Convolution (HDC).  In HDC, dilated convolutions are split into groups of three, each with a different dilation rate. Gaps between non-zero values are kept smaller than the convolutional layers’ kernels. Additionally, the different groups’ dilation rates do not have a common factor (p.34). Another solution used by augmented ASPP modules is to follow layers of increasing dilation rates with layers of decreasing dilation rates in 3x3 convolutional branches. There are other solutions like Large Kernel Pyramid Pooling (LKPP) which use three Hybrid Asymmetric Dilated Convolution (HADC) blocks, and a modified version where gridding is minimized by ensuring that subsequent dilation rates are never more than their kernel size (p.35).</font>**"
      ],
      "metadata": {
        "id": "yjYnItJmecP5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkyIMgY6zCsj"
      },
      "source": [
        "### **Q2.**\n",
        "\n",
        "Compare and contrast the usage of parallel and serial arrangements of dilated convolution layers for increasing the receptive field in a deep neural network. How does the arrangement affect the size and shape of the receptive field, and what are some benefits and drawbacks of each approach?\n",
        "\n",
        "**Hint:** There are two main ways to arrange dilated convolution layers in a network: in parallel or in series. In the parallel arrangement, multiple dilated convolutions are applied in parallel to the same input feature map, and their outputs are concatenated. In the serial arrangement, multiple dilated convolutions are applied in sequence, with the output of one layer serving as the input to the next layer. The answers to this question are available in the reading.\n",
        "\n",
        "(5 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXKsK64_nFyJ"
      },
      "source": [
        "### Answer to Q2\n",
        "\n",
        "**1. Parallel Arrangement:**\n",
        "\n",
        "In a parallel arrangement, each dilated convolution operates independently on the input feature map, leading to a simultaneous expansion of the receptive field through multiple paths.\n",
        "\n",
        "*Benefits:*\n",
        "\n",
        " - Increased Receptive Field:\n",
        "\n",
        "Parallel arrangements allow for a more rapid expansion of the receptive field compared to serial arrements, enabling the network to capture multi-scale information efficiently.\n",
        "\n",
        " - Diverse Contextual Information:\n",
        "\n",
        "Each parallel path can focus on extracting different contextual information, enhancing the network's ability to understand complex spatial relationships and object structures.\n",
        "\n",
        "*Drawbacks:*\n",
        "\n",
        " - Potential Redundancy:\n",
        "\n",
        "Running multiple dilated convolutions in parallel may introduce redundancy in feature extraction, leading to increased computational overhead without significant improvement in performance.\n",
        "\n",
        " - Complexity Management:\n",
        "\n",
        "Managing the complexity of parallel paths, especially in deep networks, can be challenging and may require careful design considerations to balance computational efficiency and performance.\n",
        "\n",
        "**2. Serial Arrangement:**\n",
        "\n",
        "In a serial arrangement, each subsequent dilated convolution layer builds upon the expanded receptive field of the previous layer, gradually increasing the network's receptive field size.\n",
        "\n",
        "*Benefits:*\n",
        "\n",
        "- Hierarchical Feature Learning:\n",
        "\n",
        "Serial arrangements promote hierarchical feature learning by sequentially incorporating contextual information from previous layers, allowing the network to capture intricate details and relationships.\n",
        "\n",
        " - Gradual Context Integration:\n",
        "\n",
        "The serial flow of information enables a gradual integration of contextual cues, facilitating a more structured and coherent understanding of the input data across multiple scales.\n",
        "\n",
        "*Drawbacks:*\n",
        "\n",
        " - Limited Parallel Processing:\n",
        "\n",
        "Serial arrangements may limit the network's ability to process diverse contextual information in parallel, potentially hindering the efficient extraction of multi-scale features and global context.\n",
        "\n",
        " - Slower Receptive Field Growth:\n",
        "\n",
        "Compared to parallel arrangements, serial configurations may exhibit slower growth in the receptive field size, requiring more layers to achieve comparable multi-scale feature representation.\n",
        "\n",
        " (sam et al., 2023 and https://chat.openai.com/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font color='red'> 4.5/5 points on Q2</font>**\n",
        "\n",
        "\n",
        " **<font color='blue'>Parallel dialated convolution can capture a large receptive field by combining information from multiple branches, which is useful for detecting irregularly shaped objects in indoor scenes how ever this architecture may not be precise when there is class imbalence. cascaded arrangement expands the receptive fields more than parallel design. This approach can better capture the global context of the image.</font>**\n"
      ],
      "metadata": {
        "id": "U_Fx_8VXKsla"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz2OmIrBlceE"
      },
      "source": [
        "## Continue with our pipeline implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiRzE4uliyfk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8639ca8f-092a-4136-c323-f94ef93f5712"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2L199Zd1mQmE"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install rasterio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8bnj2T6jZsM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "import rasterio as rio\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from IPython.core.debugger import set_trace\n",
        "from IPython.display import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJtRCLfXlb10"
      },
      "source": [
        "### Pre-process the input dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sxDF0xfnjh2"
      },
      "source": [
        "Example code is provided below, which will allow you to get up and running with this assignment. However, you will learn best if you use the code you developed/modified from previous assignments to do the work, as you will start to see how it all fits together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e603bBZ_kGG"
      },
      "source": [
        "#### Input normalization\n",
        "\n",
        "Add your own code for input normalization, or use the existing one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BffGjf8pKDbz"
      },
      "outputs": [],
      "source": [
        "def min_max_normalize_image(image, dtype=np.float32):\n",
        "    \"\"\"\n",
        "    image_path(str) : Absolute path to the image patch.\n",
        "    dtype (numpy datatype) : data type of the normalized image default is\n",
        "        \"np.float32\".\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate the minimum and maximum values for each band\n",
        "    min_values = np.nanmin(image, axis=(1, 2))[:, np.newaxis, np.newaxis]\n",
        "    max_values = np.nanmax(image, axis=(1, 2))[:, np.newaxis, np.newaxis]\n",
        "\n",
        "    # Normalize the image data to the range [0, 1]\n",
        "    normalized_img = (image - min_values) / (max_values - min_values)\n",
        "\n",
        "    # Return the normalized image data\n",
        "    return normalized_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wyfqy3J5_kGG"
      },
      "source": [
        "#### Image augmentation functions\n",
        "\n",
        "Add the functions of of your choice here, or use the code below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2lQwmu7kWP_"
      },
      "outputs": [],
      "source": [
        "def flip_image_and_label(image, label, flip_type):\n",
        "    \"\"\"\n",
        "    Applies horizontal or vertical flip augmentation to an image patch and label\n",
        "\n",
        "    Args:\n",
        "        image (numpy array) : The input image patch as a numpy array.\n",
        "        label (numpy array) : The corresponding label as a numpy array.\n",
        "        flip_type (string) : Based on the direction of flip. Can be either\n",
        "            'hflip' or 'vflip'.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the flipped image patch and label as numpy arrays.\n",
        "    \"\"\"\n",
        "    if flip_type == 'hflip':\n",
        "        # Apply horizontal flip augmentation to the image patch\n",
        "        flipped_image = cv2.flip(image, 1)\n",
        "\n",
        "        # Apply horizontal flip augmentation to the label\n",
        "        flipped_label = cv2.flip(label, 1)\n",
        "\n",
        "    elif flip_type == 'vflip':\n",
        "        # Apply vertical flip augmentation to the image patch\n",
        "        flipped_image = cv2.flip(image, 0)\n",
        "\n",
        "        # Apply vertical flip augmentation to the label\n",
        "        flipped_label = cv2.flip(label, 0)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Flip direction must be 'horizontal' or 'vertical'.\")\n",
        "\n",
        "    # Return the flipped image patch and label as a tuple\n",
        "    return flipped_image.copy(), flipped_label.copy()\n",
        "\n",
        "\n",
        "def rotate_image_and_label(image, label, angle):\n",
        "    \"\"\"\n",
        "    Applies rotation augmentation to an image patch and label.\n",
        "\n",
        "    Args:\n",
        "        image (numpy array) : The input image patch as a numpy array.\n",
        "        label (numpy array) : The corresponding label as a numpy array.\n",
        "        angle (lost of floats) : If the list has exactly two elements they will\n",
        "            be considered the lower and upper bounds for the rotation angle\n",
        "            (in degrees) respectively. If number of elements are bigger than 2,\n",
        "            then one value is chosen randomly as the roatation angle.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the rotated image patch and label as numpy arrays.\n",
        "    \"\"\"\n",
        "    if isinstance(angle, tuple) or isinstance(angle, list):\n",
        "        if len(angle) == 2:\n",
        "            rotation_degree = random.uniform(angle[0], angle[1])\n",
        "        elif len(angle) > 2:\n",
        "            rotation_degree = random.choice(angle)\n",
        "        else:\n",
        "            raise ValueError(\"Parameter degree needs at least two elements.\")\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"Rotation bound param for augmentation must be a tuple or list.\"\n",
        "        )\n",
        "\n",
        "    # Define the center of the image patch\n",
        "    center = tuple(np.array(label.shape)/2.0)\n",
        "\n",
        "    # Define the rotation matrix\n",
        "    rotation_matrix = cv2.getRotationMatrix2D(center, rotation_degree, 1.0)\n",
        "\n",
        "    # Apply rotation augmentation to the image patch\n",
        "    rotated_image = cv2.warpAffine(image, rotation_matrix, image.shape[:2],\n",
        "                                   flags=cv2.INTER_LINEAR)\n",
        "\n",
        "    # Apply rotation augmentation to the label\n",
        "    rotated_label = cv2.warpAffine(label, rotation_matrix, label.shape[:2],\n",
        "                                   flags=cv2.INTER_NEAREST)\n",
        "\n",
        "    # Return the rotated image patch and label as a tuple\n",
        "    return rotated_image.copy(), np.rint(rotated_label.copy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWYGJ57H_kGG"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7nb42nmkoDA"
      },
      "source": [
        "For assignment 4, you are working with a dataset called \"PondDataset\" which consists pairs of already chipped image and labels of size: `256x256` and pixel values are already in the range of `[0, 1]`.\n",
        "\n",
        "![Structure of the pondDataset](https://github.com/agroimpacts/adleo/blob/main/images/pond-dataset.png?raw=1 ){#fig-pond}\n",
        "\n",
        "You can find the dataset in the shared drive, which is [here](https://drive.google.com/drive/folders/1hJKRa1tNQmglErELsIEk8hXEykJadmKh?usp=share_link). Please download the entire \"PondDataset\" folder and place it in a convenient locations in your own Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Fj6IwWhnPOc"
      },
      "source": [
        "## Coding Assignment Part 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAvI5FGuzw42"
      },
      "source": [
        "This time you are provided with a pond dataset that is already chipped into tiles of `256x256` and the image values are already in the range `[0, 1]`.\n",
        "\n",
        "However there is no CSV file to read-in and load the files. You need to modify the `ActiveLoadingDataset` you have developed in assignment 3 and modify it to get the list of required \"image\" and \"label\" files directly from the stored directory. Instead of reading from a \"csv file\", you will walk through the folder structure and grab all the \"tiff\" files for \"image\" and \"label\" folders.\n",
        "\n",
        "Further instruction is provided in the corresponding answer template.\n",
        "\n",
        "(10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHBS1s04oFE1"
      },
      "source": [
        "As stated above, you can adapt the code provided below, or you can use your own loader and adapt it as needed for this assignment. In this case, you need to modify the loader so that it can read chips from a directory, rather than just reading a CSV.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKPnjJkL_kGG"
      },
      "source": [
        "### Custom dataloader\n",
        "\n",
        "Add the custom dataloader from previous assignment and modify to fit the requirements of assignment 4.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1SNASS_kD2u"
      },
      "outputs": [],
      "source": [
        "class ActiveLoadingDataset(Dataset):\n",
        "    def __init__(self, src_dir, dataset_name, usage, apply_normalization=False,\n",
        "                 transform=None, **kargs):\n",
        "        r\"\"\"\n",
        "        src_dir (str or path): Root of resource directory.\n",
        "        dataset_name (str): Name of the training/validation dataset containing\n",
        "                              structured folders for image, label\n",
        "        usage (str): Either 'train' or 'validation'.\n",
        "        transform (list): Each element is string name of the transformation to\n",
        "           be used.\n",
        "        \"\"\"\n",
        "        self.src_dir = src_dir\n",
        "        self.dataset_name = dataset_name\n",
        "        self.apply_normalization = apply_normalization\n",
        "        self.transform = transform\n",
        "\n",
        "        self.usage = usage\n",
        "        assert self.usage in [\"train\", \"validation\"], \"Usage is not recognized.\"\n",
        "\n",
        "        # Get list of Image files and sort using rglob()\n",
        "        img_dir = Path(src_dir) / self.dataset_name / self.usage / \"images\"\n",
        "\n",
        "        image_files = sorted([str(img_path)\n",
        "        for img_path in img_dir.rglob(\"*.tif\")])\n",
        "\n",
        "        # Get list of Label files and sort using rglob()\n",
        "        lbl_dir = Path(src_dir) / self.dataset_name / self.usage / \"labels\"\n",
        "\n",
        "        label_files = sorted([str(lbl_path)\n",
        "        for lbl_path in lbl_dir.rglob(\"*.tif\")])\n",
        "\n",
        "\n",
        "        self.img_chips = []\n",
        "        self.lbl_chips = []\n",
        "\n",
        "\n",
        "      # Load images and labels\n",
        "        for img_file, lbl_file in zip(image_files, label_files):\n",
        "            with rio.open(img_file) as src:\n",
        "              image = src.read().transpose(1, 2, 0)\n",
        "            with rio.open(lbl_file) as src:\n",
        "              label = src.read(1)\n",
        "\n",
        "            self.img_chips.append(image)\n",
        "            self.lbl_chips.append(label)\n",
        "\n",
        "\n",
        "        print('--------{} patches cropped--------'.format(len(self.img_chips)))\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        image_chip = self.img_chips[index]\n",
        "        label_chip = self.lbl_chips[index]\n",
        "\n",
        "        # Change the code if you are using a different augmentation\n",
        "        if self.usage == \"train\" and self.transform:\n",
        "\n",
        "            trans_flip_ls = [m for m in self.transform if \"flip\" in m]\n",
        "            if random.randint(0, 1) and len(trans_flip_ls) > 1:\n",
        "                trans_flip = random.sample(trans_flip_ls, 1)[0]\n",
        "                image_chip, label_chip = flip_image_and_label(\n",
        "                    image_chip, label_chip, trans_flip\n",
        "                )\n",
        "\n",
        "            if random.randint(0, 1) and \"rotate\" in self.transform:\n",
        "                img_chip, lbl_chip = rotate_image_and_label(\n",
        "                    image_chip, label_chip, angle=[0, 90]\n",
        "                )\n",
        "\n",
        "        # Convert numpy arrays to torch tensors.\n",
        "        image_tensor = torch.from_numpy(image_chip.transpose((2, 0, 1))).float()\n",
        "        label_tensor = torch.from_numpy(label_chip).long()\n",
        "\n",
        "\n",
        "        return image_tensor, label_tensor\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.img_chips)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2xVbJEqy2KM"
      },
      "source": [
        "#### Loading your data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKYZ7Cb9y9e4"
      },
      "outputs": [],
      "source": [
        "src_dir = \"/content/gdrive/MyDrive/adleo\"\n",
        "dataset_name = \"PondDataset\"\n",
        "\n",
        "transform = [\"hflip\", \"vflip\", \"rotate\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dGykQgXzDl6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9d3780c-1120-4242-f3b5-1543700fa827"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------359 patches cropped--------\n"
          ]
        }
      ],
      "source": [
        "train_dataset = ActiveLoadingDataset(src_dir, dataset_name, usage=\"train\",\n",
        "                                     apply_normalization=False,\n",
        "                                     transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5j3ElFJWzKw8"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size = 16,\n",
        "                          shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFJLUAfmzOlI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b07c1d1f-7dc3-4588-dbf7-d20a2dced6df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------45 patches cropped--------\n"
          ]
        }
      ],
      "source": [
        "validation_dataset = ActiveLoadingDataset(src_dir, dataset_name,\n",
        "                                          usage=\"validation\",\n",
        "                                          apply_normalization=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUMP12gmzVkT"
      },
      "outputs": [],
      "source": [
        "val_loader = DataLoader(validation_dataset, batch_size = 1, shuffle = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font color='red'> 15/15 points on CA1</font>**"
      ],
      "metadata": {
        "id": "CdgF8862N5vm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4Zo8L59l6y7"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzLyNJy1_kGG"
      },
      "source": [
        "#### Model architecture\n",
        "Provide the model architecture you developed for Assignment 3, or use the one provided below,\n",
        "a complete Unet architecture we have added for your reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "syq2_DKrmBoL"
      },
      "outputs": [],
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    r\"\"\"This module creates a user-defined number of conv+BN+ReLU layers.\n",
        "    Args:\n",
        "        in_channels (int)-- number of input features.\n",
        "        out_channels (int) -- number of output features.\n",
        "        kernel_size (int) -- Size of convolution kernel.\n",
        "        stride (int) -- decides how jumpy kernel moves along the spatial\n",
        "            dimensions.\n",
        "        padding (int) -- how much the input should be padded on the borders with\n",
        "            zero.\n",
        "        dilation (int) -- dilation ratio for enlarging the receptive field.\n",
        "        num_conv_layers (int) -- Number of conv+BN+ReLU layers in the block.\n",
        "        drop_rate (float) -- dropout rate at the end of the block.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1,\n",
        "                 padding=1, dilation=1, num_conv_layers=2, drop_rate=0):\n",
        "        super(ConvBlock, self).__init__()\n",
        "\n",
        "        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n",
        "                            stride=stride, padding=padding, dilation=dilation,\n",
        "                            bias=False),\n",
        "                  nn.BatchNorm2d(out_channels),\n",
        "                  nn.ReLU(inplace=True), ]\n",
        "\n",
        "        if num_conv_layers > 1:\n",
        "            if drop_rate > 0:\n",
        "                layers += [\n",
        "                    nn.Conv2d(out_channels, out_channels,\n",
        "                              kernel_size=kernel_size, stride=stride,\n",
        "                              padding=padding, dilation=dilation, bias=False),\n",
        "                    nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True),\n",
        "                    nn.Dropout(drop_rate),\n",
        "                ] * (num_conv_layers - 1)\n",
        "            else:\n",
        "                layers += [\n",
        "                    nn.Conv2d(out_channels, out_channels,\n",
        "                              kernel_size=kernel_size, stride=stride,\n",
        "                              padding=padding, dilation=dilation, bias=False),\n",
        "                    nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True),\n",
        "                ] * (num_conv_layers - 1)\n",
        "\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.block(inputs)\n",
        "        return outputs\n",
        "\n",
        "###########################################################################\n",
        "\n",
        "class UpconvBlock(nn.Module):\n",
        "    r\"\"\"\n",
        "    Decoder layer decodes the features along the expansive path.\n",
        "    Args:\n",
        "        in_channels (int) -- number of input features.\n",
        "        out_channels (int) -- number of output features.\n",
        "        upmode (str) -- Upsampling type.\n",
        "            If \"fixed\" then a linear upsampling with scale factor of two will be\n",
        "            applied using bi-linear as interpolation method. If deconv_1 is\n",
        "            chosen then a non-overlapping transposed convolution will be applied\n",
        "            to upsample the feature maps. If deconv_1 is chosen then an\n",
        "            overlapping transposed convolution will be applied to upsample the\n",
        "            feature maps.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, upmode=\"deconv_1\"):\n",
        "        super(UpconvBlock, self).__init__()\n",
        "\n",
        "        if upmode == \"fixed\":\n",
        "            layers = [nn.Upsample(scale_factor=2, mode=\"bilinear\",\n",
        "                                  align_corners=True), ]\n",
        "            layers += [nn.BatchNorm2d(in_channels),\n",
        "                       nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
        "                                 stride=1, padding=0, bias=False), ]\n",
        "\n",
        "        elif upmode == \"deconv_1\":\n",
        "            layers = [\n",
        "                nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2,\n",
        "                                   stride=2, padding=0, dilation=1),\n",
        "            ]\n",
        "\n",
        "        elif upmode == \"deconv_2\":\n",
        "            layers = [\n",
        "                nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4,\n",
        "                                   stride=2, padding=1, dilation=1),\n",
        "            ]\n",
        "\n",
        "        # Dense Upscaling Convolution\n",
        "        elif upmode == \"DUC\":\n",
        "            up_factor = 2\n",
        "            upsample_dim = (up_factor ** 2) * out_channels\n",
        "            layers = [nn.Conv2d(in_channels, upsample_dim, kernel_size=3,\n",
        "                                padding=1),\n",
        "                      nn.BatchNorm2d(upsample_dim),\n",
        "                      nn.ReLU(inplace=True),\n",
        "                      nn.PixelShuffle(up_factor), ]\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Provided upsampling mode is not recognized.\")\n",
        "\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return self.block(inputs)\n",
        "\n",
        "\n",
        "###########################################################################\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    def __init__(self, n_classes, in_channels, filter_config=None,\n",
        "                 dropout_rate=0):\n",
        "        super(Unet, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        if not filter_config:\n",
        "            filter_config = (64, 128, 256, 512, 1024, 2048)\n",
        "\n",
        "        assert len(filter_config) == 6\n",
        "\n",
        "        # Contraction Path\n",
        "        self.encoder_1 = ConvBlock(self.in_channels, filter_config[0],\n",
        "                                   num_conv_layers=2,\n",
        "                                   drop_rate=dropout_rate)  # 64x256x256\n",
        "        self.encoder_2 = ConvBlock(filter_config[0], filter_config[1],\n",
        "                                   num_conv_layers=2,\n",
        "                                   drop_rate=dropout_rate)  # 128x128x128\n",
        "        self.encoder_3 = ConvBlock(filter_config[1], filter_config[2],\n",
        "                                   num_conv_layers=2,\n",
        "                                   drop_rate=dropout_rate)  # 256x64x64\n",
        "        self.encoder_4 = ConvBlock(filter_config[2], filter_config[3],\n",
        "                                   num_conv_layers=2,\n",
        "                                   drop_rate=dropout_rate)  # 512x32x32\n",
        "        self.encoder_5 = ConvBlock(filter_config[3], filter_config[4],\n",
        "                                   num_conv_layers=2,\n",
        "                                   drop_rate=dropout_rate)  # 1024x16x16\n",
        "        self.encoder_6 = ConvBlock(filter_config[4], filter_config[5],\n",
        "                                   num_conv_layers=2,\n",
        "                                   drop_rate=dropout_rate)  # 2048x8x8\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Expansion Path\n",
        "        self.decoder_1 = UpconvBlock(filter_config[5], filter_config[4],\n",
        "                                     upmode=\"deconv_2\")  # 1024x16x16\n",
        "        self.conv1 = ConvBlock(filter_config[4] * 2, filter_config[4],\n",
        "                               num_conv_layers=2, drop_rate=dropout_rate)\n",
        "\n",
        "        self.decoder_2 = UpconvBlock(filter_config[4], filter_config[3],\n",
        "                                     upmode=\"deconv_2\")  # 512x32x32\n",
        "        self.conv2 = ConvBlock(filter_config[4], filter_config[3],\n",
        "                               num_conv_layers=2, drop_rate=dropout_rate)\n",
        "\n",
        "        self.decoder_3 = UpconvBlock(filter_config[3], filter_config[2],\n",
        "                                     upmode=\"deconv_2\")  # 256x64x64\n",
        "        self.conv3 = ConvBlock(filter_config[3], filter_config[2],\n",
        "                               num_conv_layers=2, drop_rate=dropout_rate)\n",
        "\n",
        "        self.decoder_4 = UpconvBlock(filter_config[2], filter_config[1],\n",
        "                                     upmode=\"deconv_2\")  # 128x128x128\n",
        "        self.conv4 = ConvBlock(filter_config[2], filter_config[1],\n",
        "                               num_conv_layers=2, drop_rate=dropout_rate)\n",
        "\n",
        "        self.decoder_5 = UpconvBlock(filter_config[1], filter_config[0],\n",
        "                                     upmode=\"deconv_2\")  # 64x256x256\n",
        "        self.conv5 = ConvBlock(filter_config[1], filter_config[0],\n",
        "                               num_conv_layers=2, drop_rate=dropout_rate)\n",
        "\n",
        "        self.classifier = nn.Conv2d(filter_config[0], n_classes, kernel_size=1,\n",
        "                                    stride=1, padding=0)  # classNumx256x256\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # set_trace()\n",
        "        e1 = self.encoder_1(inputs)  # batch size x 64 x 256 x 256\n",
        "        p1 = self.pool(e1)  # batch size x 64 x 128 x 128\n",
        "\n",
        "        e2 = self.encoder_2(p1)  # batch size x 128 x 128 x 128\n",
        "        p2 = self.pool(e2)  # batch size x 128 x 64 x 64\n",
        "\n",
        "        e3 = self.encoder_3(p2)  # batch size x 256 x 64 x 64\n",
        "        p3 = self.pool(e3)  # batch size x 256 x 32 x 32\n",
        "\n",
        "        e4 = self.encoder_4(p3)  # batch size x 512 x 32 x 32\n",
        "        p4 = self.pool(e4)  # batch size x 1024 x 16 x 16\n",
        "\n",
        "        e5 = self.encoder_5(p4)  # batch size x 1024 x 16 x 16\n",
        "        p5 = self.pool(e5)  # batch size x 1024 x 8 x 8\n",
        "\n",
        "        e6 = self.encoder_6(p5)  # batch size x 2048 x 8 x 8\n",
        "\n",
        "        d6 = self.decoder_1(e6)  # batch size x 1024 x 16 x 16\n",
        "        skip1 = torch.cat((e5, d6), dim=1)  # batch size x 2048 x 16 x 16\n",
        "        d6_proper = self.conv1(skip1)  # batch size x 1024 x 16 x 16\n",
        "\n",
        "        d5 = self.decoder_2(d6_proper)  # batch size x 512 x 32 x 32\n",
        "        skip2 = torch.cat((e4, d5), dim=1)  # batch size x 1024 x 32 x 32\n",
        "        d5_proper = self.conv2(skip2)  # batch size x 512 x 32 x 32\n",
        "\n",
        "        d4 = self.decoder_3(d5_proper)  # batch size x 256 x 64 x 64\n",
        "        skip3 = torch.cat((e3, d4), dim=1)  # batch size x 512 x 64 x 64\n",
        "        d4_proper = self.conv3(skip3)  # batch size x 256 x 64 x 64\n",
        "\n",
        "        d3 = self.decoder_4(d4_proper)  # batch size x 128 x 128 x 128\n",
        "        skip4 = torch.cat((e2, d3), dim=1)  # batch size x 256 x 128 x 128\n",
        "        d3_proper = self.conv4(skip4)  # batch size x 128 x 128 x 128\n",
        "\n",
        "        d2 = self.decoder_5(d3_proper)  # batch size x 64 x 256 x 256\n",
        "        skip5 = torch.cat((e1, d2), dim=1)  # batch size x 128 x 256 x 256\n",
        "        d2_proper = self.conv5(skip5)  # batch size x 64 x 256 x 256\n",
        "\n",
        "        d1 = self.classifier(d2_proper)  # batch size x classNum x 256 x 256\n",
        "\n",
        "        return d1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class unet_sus(nn.Module):\n",
        "    def __init__(self, n_classes, in_channels, filter_config=(64, 128, 256, 512, 1024), dropout_rate=0):\n",
        "        super(unet_sus, self).__init__()\n",
        "        self.n_classes = n_classes\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # Remove redundant redefinitions\n",
        "        # filter_config = (64, 128, 256, 512, 1024)\n",
        "        # dropout_rate = 0\n",
        "\n",
        "        # Downsample\n",
        "        # 1st Block\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, filter_config[0], kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(filter_config[0]),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # 2nd Block\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(filter_config[0], filter_config[1], kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(filter_config[1]),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # 3rd Block\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(filter_config[1], filter_config[2], kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(filter_config[2]),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # 4th Block\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(filter_config[2], filter_config[3], kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(filter_config[3]),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # 5th Block\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(filter_config[3], filter_config[4], kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(filter_config[4]),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # 6th Block (BottleNeck)\n",
        "        self.conv6 = nn.Sequential(\n",
        "            nn.Conv2d(filter_config[4], filter_config[4], kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(filter_config[4]),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Upsample\n",
        "        # 7th Block\n",
        "        self.us1 = nn.ConvTranspose2d(filter_config[4], filter_config[4], kernel_size=2, stride=2)\n",
        "        self.conv7 = nn.Sequential(\n",
        "            nn.Conv2d(filter_config[4], filter_config[3], kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(filter_config[3]),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.us2 = nn.ConvTranspose2d(filter_config[3], filter_config[3], kernel_size=2, stride=2)\n",
        "        # 8th Block\n",
        "        self.conv8 = nn.Sequential(\n",
        "            nn.Conv2d(filter_config[3], filter_config[2], kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(filter_config[2]),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.us3 = nn.ConvTranspose2d(filter_config[2], filter_config[2], kernel_size=2, stride=2)\n",
        "        # 9th Block\n",
        "        self.conv9 = nn.Sequential(\n",
        "            nn.Conv2d(filter_config[2], filter_config[1], kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(filter_config[1]),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.us4 = nn.ConvTranspose2d(filter_config[1], filter_config[1], kernel_size=2, stride=2)\n",
        "        # 10th Block\n",
        "        self.conv10 = nn.Sequential(\n",
        "            nn.Conv2d(filter_config[1], filter_config[0], kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(filter_config[0]),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Correct misnamed layer\n",
        "        self.us5 = nn.ConvTranspose2d(filter_config[0], filter_config[0], kernel_size=2, stride=2)\n",
        "\n",
        "        # 11th Block\n",
        "        self.conv11 = nn.Sequential(\n",
        "            nn.Conv2d(filter_config[0], n_classes, kernel_size=3, stride=1, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Downsample\n",
        "        self.ds = nn.MaxPool2d(2, stride=2)\n",
        "\n",
        "        dslyr1 = self.conv1(inputs)\n",
        "        ds1 = self.ds(dslyr1)\n",
        "\n",
        "        dslyr2 = self.conv2(ds1)\n",
        "        ds2 = self.ds(dslyr2)\n",
        "\n",
        "        dslyr3 = self.conv3(ds2)\n",
        "        ds3 = self.ds(dslyr3)\n",
        "\n",
        "        dslyr4 = self.conv4(ds3)\n",
        "        ds4 = self.ds(dslyr4)\n",
        "\n",
        "        dslyr5 = self.conv5(ds4)\n",
        "        ds5 = self.ds(dslyr5)\n",
        "\n",
        "        # Bottleneck\n",
        "        btlnklyr = self.conv6(ds5)\n",
        "\n",
        "        # Upsample\n",
        "        us1 = self.us1(btlnklyr)\n",
        "        ulyr1 = self.conv7(torch.cat([us1, dslyr5], 1))\n",
        "\n",
        "        us2 = self.us2(ulyr1)\n",
        "        merge6 = torch.cat([us2, dslyr4], 1)\n",
        "        ulyr2 = self.conv8(merge6)\n",
        "\n",
        "        us3 = self.us3(ulyr2)\n",
        "        merge7 = torch.cat([us3, dslyr3], 1)\n",
        "        ulyr3 = self.conv9(merge7)\n",
        "\n",
        "        us4 = self.us4(ulyr3)\n",
        "        merge8 = torch.cat([us4, dslyr2], 1)\n",
        "        ulyr4 = self.conv10(merge8)\n",
        "\n",
        "        us5 = self.us5(ulyr4)\n",
        "        merge9 = torch.cat([us5, dslyr1], 1)\n",
        "        ulyr5 = self.conv11(merge9)\n",
        "        output = self.output(ulyr5)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "cGCs4ke428Vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWdmgmPIzeE-"
      },
      "source": [
        "#### Initialize your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91DbioGYzluC"
      },
      "outputs": [],
      "source": [
        "n_classes = 2\n",
        "in_channels = 6\n",
        "filter_config = (32, 64, 128, 256, 512, 1024)\n",
        "dropout_rate = 0.15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qtiayi6AzoA-"
      },
      "outputs": [],
      "source": [
        "model = Unet(n_classes, in_channels, filter_config, dropout_rate)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sus_model = unet_sus(n_classes, in_channels, filter_config, dropout_rate)"
      ],
      "metadata": {
        "id": "XLcVqupW3FPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4hMpDQ1ma0D"
      },
      "source": [
        "### Customized loss function\n",
        "\n",
        "You will want to add two here, which you can copy from the main assignment notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "H8fe9coXmgOh"
      },
      "outputs": [],
      "source": [
        "class BalancedCrossEntropyLoss(nn.Module):\n",
        "    '''\n",
        "    Balanced cross entropy loss by weighting of inverse class ratio\n",
        "    Params:\n",
        "        ignore_index (int): Class index to ignore\n",
        "        reduction (str): Reduction method to apply to loss, return mean over batch if 'mean',\n",
        "            return sum if 'sum', return a tensor of shape [N,] if 'none'\n",
        "    Returns:\n",
        "        Loss tensor according to arg reduction\n",
        "    '''\n",
        "\n",
        "    def __init__(self, ignore_index=-100, reduction='mean'):\n",
        "        super(BalancedCrossEntropyLoss, self).__init__()\n",
        "        self.ignore_index = ignore_index\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, predict, target):\n",
        "        # get class weights\n",
        "        class_counts = torch.bincount(target.view(-1),\n",
        "                                      minlength=predict.shape[1])\n",
        "        class_weights = 1.0 / torch.sqrt(class_counts.float())\n",
        "\n",
        "        # set weight of ignore index to 0\n",
        "        if self.ignore_index >= 0 and self.ignore_index < len(class_weights):\n",
        "            class_weights[self.ignore_index] = 0\n",
        "\n",
        "        # normalize weights\n",
        "        class_weights /= torch.sum(class_weights)\n",
        "\n",
        "        # apply class weights to loss function\n",
        "        loss_fn = nn.CrossEntropyLoss(weight=class_weights,\n",
        "                                      ignore_index=self.ignore_index,\n",
        "                                      reduction=self.reduction)\n",
        "\n",
        "        return loss_fn(predict, target)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BinaryDiceLoss(nn.Module):\n",
        "    '''\n",
        "        Dice loss of binary class\n",
        "        Params:\n",
        "            smooth (float): A float number to smooth loss, and avoid NaN error,\n",
        "              default: 1\n",
        "            p (int): Denominator value: \\sum{x^p} + \\sum{y^p}, default: 2. Used\n",
        "              to control the sensitivity of the loss.\n",
        "            predict (torch.tensor): Predicted tensor of shape [N, *]\n",
        "            target (torch.tensor): Target tensor of same shape with predict\n",
        "        Returns:\n",
        "            Loss tensor\n",
        "    '''\n",
        "    def __init__(self, smooth=1, p=1):\n",
        "        super(BinaryDiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, predict, target):\n",
        "\n",
        "        assert predict.shape == target.shape, \"predict & target don't match\"\n",
        "        predict = predict.contiguous().view(-1)\n",
        "        target = target.contiguous().view(-1)\n",
        "\n",
        "        num = 2 * (predict * target).sum() + self.smooth\n",
        "        den = (predict.pow(self.p) + target.pow(self.p)).sum() + self.smooth\n",
        "        loss = 1 - num / den\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    '''\n",
        "        Dice loss\n",
        "        Params:\n",
        "            weight (torch.tensor): Weight array of shape [num_classes,]\n",
        "            ignore_index (int): Class index to ignore\n",
        "            predict (torch.tensor): Predicted tensor of shape [N, C, *]\n",
        "            target (torch.tensor): Target tensor either in shape [N,*] or of\n",
        "              same shape with predict\n",
        "            reduction (str): Reduction method.\n",
        "        Returns:\n",
        "            same as BinaryDiceLoss\n",
        "    '''\n",
        "    def __init__(self, weight=None, ignore_index=-100, smooth=1, p=1,\n",
        "                 reduction='sum'):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.ignore_index = ignore_index\n",
        "        self.reduction = reduction\n",
        "        self.dice = BinaryDiceLoss(smooth, p)\n",
        "        if weight is not None:\n",
        "            self.weight = weight.cuda()\n",
        "        else:\n",
        "            self.weight = None\n",
        "\n",
        "    def forward(self, predict, target):\n",
        "        #set_trace()\n",
        "        if predict.shape == target.shape:\n",
        "            pass\n",
        "        elif len(predict.shape) == 4:\n",
        "            target = F.one_hot(target, num_classes=predict.shape[1])\\\n",
        "                .permute(0, 3, 1, 2)\\\n",
        "                .contiguous()\n",
        "        else:\n",
        "            assert 'predict shape not applicable'\n",
        "\n",
        "        self.weight = torch.Tensor([1. / predict.shape[1]] * predict.shape[1])\\\n",
        "            .cuda() if self.weight is None else self.weight\n",
        "        predict = F.softmax(predict, dim=1)\n",
        "\n",
        "        if self.ignore_index >= 0:\n",
        "            target = torch.where(target == self.ignore_index,\n",
        "                                 torch.zeros_like(target), target)\n",
        "            predict = torch.where(target == self.ignore_index,\n",
        "                                  torch.zeros_like(predict), predict)\n",
        "\n",
        "        total_loss = 0\n",
        "        for i in range(predict.shape[1]):\n",
        "            dice_loss = self.dice(predict[:, i], target[:, i])\n",
        "\n",
        "            assert self.weight.shape[0] == predict.shape[1], \\\n",
        "                    'Expect weight shape [{}], get[{}]'\\\n",
        "                    .format(predict.shape[1], self.weight.shape[0])\n",
        "            dice_loss *= self.weight[i]\n",
        "            total_loss += dice_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            loss = total_loss / predict.shape[1]\n",
        "        elif self.reduction == 'sum':\n",
        "            loss = total_loss\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "fVLhtxTwIYSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BalancedCrossEntropyLossOHEM(nn.Module):\n",
        "    \"\"\"\n",
        "    Online Hard Example Mining (OHEM) Cross Entropy Loss for Semantic\n",
        "    Segmentation\n",
        "    Params:\n",
        "        ignore_index (int): Class index to ignore\n",
        "        reduction (str): Reduction method to apply to loss, return mean over\n",
        "            batch if 'mean', return sum if 'sum', return a tensor of shape [N,]\n",
        "            if 'none'\n",
        "        ohem_ratio (float): Ratio of hard examples to use in the loss function\n",
        "    Returns:\n",
        "        Loss tensor according to arg reduction\n",
        "    \"\"\"\n",
        "    def __init__(self, ignore_index=-100, reduction='mean', ohem_ratio=0.25):\n",
        "        super(BalancedCrossEntropyLossOHEM, self).__init__()\n",
        "        self.ignore_index = ignore_index\n",
        "        self.reduction = reduction\n",
        "        self.ohem_ratio = ohem_ratio\n",
        "\n",
        "    def forward(self, predict, target):\n",
        "        # calculate pixel-wise cross entropy loss\n",
        "        loss_fn = nn.CrossEntropyLoss(ignore_index=self.ignore_index,\n",
        "                                      reduction='none')\n",
        "        pixel_losses = loss_fn(predict, target)\n",
        "\n",
        "        # apply online hard example mining\n",
        "        num_hard = int(self.ohem_ratio * pixel_losses.numel())\n",
        "        _, top_indices = pixel_losses.flatten().topk(num_hard)\n",
        "        ohem_losses = pixel_losses.flatten()[top_indices]\n",
        "\n",
        "        # apply reduction to ohem losses\n",
        "        if self.reduction == 'mean':\n",
        "            loss = ohem_losses.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            loss = ohem_losses.sum()\n",
        "        else:\n",
        "            loss = ohem_losses\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "E-9k06YYD5tJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8M9vkOj7AT86"
      },
      "source": [
        "## Coding assignment part 2: Training the network\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eh13sfNJvDJy"
      },
      "source": [
        "\n",
        "In the sections below you need to complete functions that you need to train and validate the network over a specified number of epochs\n",
        "\n",
        "Complete the training process. To do that you need to complete three functions.\n",
        "\n",
        "1- A function to perform one epoch on the training dataset.\n",
        "\n",
        "2- A function to perform one epoch on the validation dataset.\n",
        "\n",
        "3- A function to iterate over the user-defined number of epochs\n",
        "\n",
        "Develop this code and train/validate the model two times, each with a different loss function.\n",
        "\n",
        "**Note:**\n",
        "Detailed information is provided in the answer template.\n",
        "\n",
        "(15 points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWRrySwHBBT4"
      },
      "source": [
        "**Tip:**\n",
        "\n",
        "Q. How do you properly use the \"criterion\" argument inside the \"train\" and \"validation\" functions?\n",
        "\n",
        "A. Pass the argument to the function as a string with `()` like: \"BalancedCrossEntropyLoss()\". Then, when it comes to using the argument inside both the `train` and `validation` functions, use `eval()` like this:\n",
        "\n",
        "`loss = eval(criterion)(tensor A, tensor B)`\n",
        "\n",
        "`eval()` is a built-in Python function that allows you to evaluate a string expression as a Python code. It takes a string as an argument and evaluates the expression contained in it. The result of the evaluation is then returned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZYDPXNq_kGK"
      },
      "source": [
        "### Complete the `train` function\n",
        "\n",
        "Complete the function to optimize over a batch of training images and labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-AEldksAikB"
      },
      "outputs": [],
      "source": [
        "def train(trainData, model, optimizer, criterion, gpu=True, train_loss=[]):\n",
        "    \"\"\"\n",
        "        Train the model using provided training dataset.\n",
        "        Params:\n",
        "            trainData (DataLoader object) -- Batches of image chips from PyTorch\n",
        "                custom dataset (AquacultureData).\n",
        "            model - Choice of segmentation model.\n",
        "            optimizer - Chosen optimization algorithm to update model parameters.\n",
        "            criterion - Chosen function to calculate loss over training samples.\n",
        "            gpu (bool, optional) -- Decide whether to use GPU, default is True.\n",
        "            train_loss (empty list, optional) -- Stores average epoch loss for\n",
        "            history.\n",
        "    \"\"\"\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Mini batch iteration\n",
        "    train_epoch_loss = 0\n",
        "    train_batches = len(trainData)\n",
        "\n",
        "    device = torch.device(\"cuda\") if gpu and torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "    for img_chips, labels in trainData:\n",
        "\n",
        "        # Add code to put image and label on the 'device'.\n",
        "        img_chips = img_chips.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Add code to clear the 'optimizer' from existing gradients (1 line)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Pass image through the model to obtain prediction (1 line)\n",
        "        prediction = model(img_chips)\n",
        "\n",
        "        # calculate loss based on 'model prediction' and label (1 line)\n",
        "        loss = criterion(prediction, labels)\n",
        "\n",
        "        # Add current loss (loss.item()) to 'train_epoch_loss' counter (1 line)\n",
        "        train_epoch_loss += loss.item()\n",
        "\n",
        "        # do the backward pass to calculate gradients with respect to the loss\n",
        "        loss.backward()\n",
        "\n",
        "        # (1 line) update model weights by invoking the proper method on\n",
        "        # 'optimizer'\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss.append(train_epoch_loss / train_batches)\n",
        "    print('Training loss: {:.4f}'.format(train_epoch_loss / train_batches))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvQWD_wTs6lT"
      },
      "source": [
        "### Complete the `validation` function\n",
        "Besides training the network, it's important to evaluate its performance on a separate \"validation dataset\" to ensure that it's not overfitting to the training data. The validation process is similar to the training process, but the network is set to evaluation mode using `model.eval()` and the gradients are not computed.\n",
        "\n",
        "Complete the function to process validation images and labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "l2VxqLHiBDFi"
      },
      "outputs": [],
      "source": [
        "def validate(valData, model, criterion, device, val_loss=[]):\n",
        "    \"\"\"\n",
        "        Evaluate the model on separate Landsat scenes.\n",
        "        Params:\n",
        "            valData (DataLoader object) -- Batches of image chips from PyTorch\n",
        "                custom dataset(AquacultureData)\n",
        "            model -- Choice of segmentation Model.\n",
        "            criterion -- Chosen function to calculate loss over validation\n",
        "                samples.\n",
        "            buffer: Buffer added to the targeted grid when creating dataset.\n",
        "                This allows loss to calculate at non-buffered region.\n",
        "            gpu (binary,optional): Decide whether to use GPU, default is True\n",
        "            valLoss (empty list): To record average loss for each epoch\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # mini batch iteration\n",
        "    eval_epoch_loss = 0\n",
        "\n",
        "    for img_chips, labels in valData:\n",
        "\n",
        "        img = Variable(img_chips, requires_grad=False)\n",
        "        label = Variable(labels, requires_grad=False)\n",
        "\n",
        "        #Add code to put image and label on the 'device'.\n",
        "        # one line for each.\n",
        "        img_chips = img.to(device)\n",
        "        labels = label.to(device)\n",
        "\n",
        "        # Pass image through the model to obtain prediction (1 line)\n",
        "        prediction = model(img_chips)\n",
        "\n",
        "        # calculate loss based on 'model prediction' and label (1 line)\n",
        "        loss = criterion(prediction, labels)\n",
        "\n",
        "        # Add current loss (loss.item()) to 'train_epoch_loss' counter (1 line)\n",
        "        eval_epoch_loss += loss.item()\n",
        "\n",
        "    print('validation loss: {}'.format(eval_epoch_loss / len(valData)))\n",
        "\n",
        "    if val_loss != None:\n",
        "        val_loss.append(float(eval_epoch_loss / len(valData)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s33xS7UB_kGK"
      },
      "source": [
        "### Complete the epochIterator\n",
        "\n",
        "Complete the function that iterate over the desired number of epochs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TQFVUtNtD2s_"
      },
      "outputs": [],
      "source": [
        "def epochIterater(trainData, valData, model, criterion, WorkingFolder,\n",
        "                  initial_lr, num_epochs):\n",
        "    r\"\"\"\n",
        "    Epoch iteration for train and evaluation.\n",
        "\n",
        "    Arguments:\n",
        "    trainData (dataloader object): Batch grouped data to train the model.\n",
        "    evalData (dataloader object): Batch grouped data to evaluate the model.\n",
        "    model (pytorch.nn.module object): initialized model.\n",
        "    initial_lr(float): The initial learning rate.\n",
        "    num_epochs (int): User-defined number of epochs to run the model.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    train_loss = []\n",
        "    val_loss = []\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    if device.type == \"cuda\":\n",
        "        print('----------GPU available----------')\n",
        "        gpu = True\n",
        "        model = model.to(device)\n",
        "    else:\n",
        "        print('----------No GPU available, using CPU instead----------')\n",
        "        gpu = False\n",
        "        model = model\n",
        "\n",
        "    writer = SummaryWriter(WorkingFolder)\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                           lr=initial_lr,\n",
        "                           betas=(0.9, 0.999),\n",
        "                           eps=1e-08,\n",
        "                           weight_decay=5e-4,\n",
        "                           amsgrad=False)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
        "                                          step_size=3,\n",
        "                                          gamma=0.98)\n",
        "\n",
        "    # Add your code here\n",
        "    # you need to loop through the epochs and perform the following:\n",
        "    # print the current epoch number out of the total epochs\n",
        "    # (e.g. \"epoch: 2/10\")(1 line)\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "      print(f\"Epoch: {epoch}/{num_epochs}\")\n",
        "\n",
        "      # start the timer (1 line)\n",
        "      start_epoch = datetime.now()\n",
        "\n",
        "      # do model fit on the training data for single epoch (1 line)\n",
        "      train(trainData, model, optimizer, criterion, gpu, train_loss)\n",
        "\n",
        "      # do model validation on the validation dataset for one epoch (1 line)\n",
        "      validate(valData, model, criterion, device, val_loss)\n",
        "\n",
        "      # take a step to update the 'scheduler'. (1 line)\n",
        "      scheduler.step()\n",
        "\n",
        "      # Print the updated learning rate.\n",
        "      print(f\"Updated Learning Rate: {scheduler.get_lr()[0]}\")\n",
        "\n",
        "    # use \"add_scalars\" method with your writer to save the train and validation\n",
        "    # loss to graph\n",
        "    # using tensorboard package later.\n",
        "      writer.add_scalars(\"Loss\", {\"Train\": train_loss[-1],\n",
        "                                \"Val\": val_loss[-1]}, epoch)\n",
        "\n",
        "      writer.close()\n",
        "\n",
        "    duration_in_sec = (datetime.now() - start_epoch).seconds\n",
        "    duration_format = str(timedelta(seconds=duration_in_sec))\n",
        "    print(\"--------------- Training finished in {} ---------------\"\\\n",
        "          .format(duration_format))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pG9Ni2kwKC5"
      },
      "source": [
        "### Demonstrate the code\n",
        "Run the model training and validation for a specified number of epochs (e.g. 15), and then save the results. Train / validate twice, once using your first loss function, and again using your second loss function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0X38xlEs_kGK"
      },
      "source": [
        "#### Train/validate model 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wq1-0zAKyLQz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9645982f-6909-4221-a4d1-6a9bc5ca5bc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------GPU available----------\n",
            "Epoch: 1/10\n",
            "Training loss: 0.6030\n",
            "validation loss: 0.7046905544069079\n",
            "Updated Learning Rate: 0.0001\n",
            "Epoch: 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 0.4172\n",
            "validation loss: 0.6201394650671217\n",
            "Updated Learning Rate: 0.0001\n",
            "Epoch: 3/10\n",
            "Training loss: 0.3570\n",
            "validation loss: 0.5442100604375203\n",
            "Updated Learning Rate: 9.604e-05\n",
            "Epoch: 4/10\n",
            "Training loss: 0.3302\n",
            "validation loss: 0.5223104748460982\n",
            "Updated Learning Rate: 9.8e-05\n",
            "Epoch: 5/10\n",
            "Training loss: 0.3155\n",
            "validation loss: 0.4902788506613837\n",
            "Updated Learning Rate: 9.8e-05\n",
            "Epoch: 6/10\n",
            "Training loss: 0.3119\n",
            "validation loss: 0.4906127005815506\n",
            "Updated Learning Rate: 9.41192e-05\n",
            "Epoch: 7/10\n",
            "Training loss: 0.3025\n",
            "validation loss: 0.4797595477766461\n",
            "Updated Learning Rate: 9.604e-05\n",
            "Epoch: 8/10\n",
            "Training loss: 0.2849\n",
            "validation loss: 0.47246027721299066\n",
            "Updated Learning Rate: 9.604e-05\n",
            "Epoch: 9/10\n",
            "Training loss: 0.2712\n",
            "validation loss: 0.47852570248974696\n",
            "Updated Learning Rate: 9.2236816e-05\n",
            "Epoch: 10/10\n",
            "Training loss: 0.2627\n",
            "validation loss: 0.5050224443276723\n",
            "Updated Learning Rate: 9.41192e-05\n",
            "--------------- Training finished in 0:00:03 ---------------\n"
          ]
        }
      ],
      "source": [
        "# Train/validate 1 using distribution-based class-entropy loss function\n",
        "trainData = train_loader\n",
        "valData = val_loader\n",
        "model = model\n",
        "criterion = BalancedCrossEntropyLoss()\n",
        "WorkingFolder = \"/content/gdrive/MyDrive/adleo/assignment4\"\n",
        "initial_lr = 1e-4\n",
        "num_epochs = 10\n",
        "epochIterater(trainData, valData, model, criterion, WorkingFolder, initial_lr, num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkYNukTM_kGK"
      },
      "source": [
        "Save model 1 in a directory of choice in your gdrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bLd8KNWID22z"
      },
      "outputs": [],
      "source": [
        "# Save model results 1\n",
        "BCEL_model_path = \"/content/gdrive/MyDrive/adleo/assignment4/BCEL_model.pth\"\n",
        "torch.save(model.state_dict(), BCEL_model_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z32cn7OR_kGK"
      },
      "source": [
        "#### Train/validate model 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "P9zRPuuCyZlg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25f26b03-fd79-4e3a-f4f9-0cb3c39456b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------GPU available----------\n",
            "Epoch: 1/10\n",
            "Training loss: 0.5923\n",
            "validation loss: 0.8764635026454926\n",
            "Updated Learning Rate: 0.0001\n",
            "Epoch: 2/10\n",
            "Training loss: 0.5645\n",
            "validation loss: 0.8778440806600782\n",
            "Updated Learning Rate: 0.0001\n",
            "Epoch: 3/10\n",
            "Training loss: 0.5475\n",
            "validation loss: 0.9114678819974263\n",
            "Updated Learning Rate: 9.604e-05\n",
            "Epoch: 4/10\n",
            "Training loss: 0.5379\n",
            "validation loss: 0.9050062212679121\n",
            "Updated Learning Rate: 9.8e-05\n",
            "Epoch: 5/10\n",
            "Training loss: 0.5200\n",
            "validation loss: 0.9216080947054757\n",
            "Updated Learning Rate: 9.8e-05\n",
            "Epoch: 6/10\n",
            "Training loss: 0.5004\n",
            "validation loss: 0.9634029060602188\n",
            "Updated Learning Rate: 9.41192e-05\n",
            "Epoch: 7/10\n",
            "Training loss: 0.5192\n",
            "validation loss: 0.9745307981967926\n",
            "Updated Learning Rate: 9.604e-05\n",
            "Epoch: 8/10\n",
            "Training loss: 0.5012\n",
            "validation loss: 0.9438784446981218\n",
            "Updated Learning Rate: 9.604e-05\n",
            "Epoch: 9/10\n",
            "Training loss: 0.4873\n",
            "validation loss: 0.9122659802436829\n",
            "Updated Learning Rate: 9.2236816e-05\n",
            "Epoch: 10/10\n",
            "Training loss: 0.4810\n",
            "validation loss: 0.9047593043910133\n",
            "Updated Learning Rate: 9.41192e-05\n",
            "--------------- Training finished in 0:00:03 ---------------\n"
          ]
        }
      ],
      "source": [
        "# Train/validate 2 using online hard example mining based loss function\n",
        "trainData = train_loader\n",
        "valData = val_loader\n",
        "model = model\n",
        "criterion = BalancedCrossEntropyLossOHEM()\n",
        "WorkingFolder = \"/content/gdrive/MyDrive/adleo/assignment4\"\n",
        "initial_lr = 1e-4\n",
        "num_epochs = 10\n",
        "epochIterater(trainData, valData, model, criterion, WorkingFolder, initial_lr, num_epochs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB5IhSxY_kGK"
      },
      "source": [
        "Save model results 2 in a directory of choice in your gdrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kyrDM0hqyeMz"
      },
      "outputs": [],
      "source": [
        "# Save model results 2\n",
        "OHEM_model_path = \"/content/gdrive/MyDrive/adleo/assignment4/OHEM_model.pth\"\n",
        "torch.save(model.state_dict(), OHEM_model_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font color='red'> 15/15 points on CA2</font>**"
      ],
      "metadata": {
        "id": "Upg9nQgeOF3e"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaCaNlvonXp4"
      },
      "source": [
        "## Evaluation and accuracy metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey2Ep2LAoRCX"
      },
      "source": [
        "**Note:**\n",
        "\n",
        "If you have disconnected from the Colab session or restarted the kernel, then before doing the evaluation on the validation dataset you must initialize your model once more and load the trained weights onto your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MLovzg7NngXX"
      },
      "outputs": [],
      "source": [
        "class Evaluator(object):\n",
        "    def __init__(self, num_class):\n",
        "        self.num_class = num_class\n",
        "        self.confusion_matrix = np.zeros((self.num_class,)*2)\n",
        "\n",
        "    def Pixel_Accuracy(self):\n",
        "        Acc = np.diag(self.confusion_matrix).sum() / self.confusion_matrix.sum()\n",
        "        return Acc\n",
        "\n",
        "    def Pixel_Accuracy_Class(self):\n",
        "        Acc = np.diag(self.confusion_matrix) / self.confusion_matrix.sum(axis=1)\n",
        "        Acc = np.nanmean(Acc)\n",
        "        return Acc\n",
        "\n",
        "    def Mean_Intersection_over_Union(self):\n",
        "        MIoU = np.diag(self.confusion_matrix) / (\n",
        "                    np.sum(self.confusion_matrix, axis=1) +\n",
        "                    np.sum(self.confusion_matrix, axis=0) -\n",
        "                    np.diag(self.confusion_matrix))\n",
        "        MIoU = np.nanmean(MIoU)\n",
        "        return MIoU\n",
        "\n",
        "    def Frequency_Weighted_Intersection_over_Union(self):\n",
        "        freq = np.sum(self.confusion_matrix, axis=1) /\\\n",
        "            np.sum(self.confusion_matrix)\n",
        "        iu = np.diag(self.confusion_matrix) / (\n",
        "                    np.sum(self.confusion_matrix, axis=1) +\n",
        "                    np.sum(self.confusion_matrix, axis=0) -\n",
        "                    np.diag(self.confusion_matrix))\n",
        "\n",
        "        FWIoU = (freq[freq > 0] * iu[freq > 0]).sum()\n",
        "        return FWIoU\n",
        "\n",
        "    def _generate_matrix(self, gt_image, pre_image):\n",
        "        mask = (gt_image >= 0) & (gt_image < self.num_class)\n",
        "        label = self.num_class * gt_image[mask].astype('int') + pre_image[mask]\n",
        "        count = np.bincount(label, minlength=self.num_class**2)\n",
        "        confusion_matrix = count.reshape(self.num_class, self.num_class)\n",
        "        return confusion_matrix\n",
        "\n",
        "    def add_batch(self, gt_image, pre_image):\n",
        "        assert gt_image.shape == pre_image.shape\n",
        "        self.confusion_matrix += self._generate_matrix(gt_image, pre_image)\n",
        "\n",
        "    def reset(self):\n",
        "        self.confusion_matrix = np.zeros((self.num_class,) * 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kux73Vkuy6bn"
      },
      "source": [
        "## Coding Assignment Part 3\n",
        "\n",
        "Modify `do_accuracy_evaluation` to work with the `Evaluator` class to calculate the overal metrics for a validation dataset.\n",
        "\n",
        "\n",
        "More info on the specification of the function can be found in the template\n",
        "\n",
        "\n",
        "Complete the code to undertake model evaluation below. Evaluate twice: once for each model trained with a different loss function.\n",
        "\n",
        "(10 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypJtuQOT_kGL"
      },
      "source": [
        "### Add the code for evaluation here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYZ4CIs5no54"
      },
      "outputs": [],
      "source": [
        "def do_accuracy_evaluation(model, dataloader, num_classes, filename):\n",
        "    \"\"\"\n",
        "  Evaluates the model's accuracy on a given dataset.\n",
        "\n",
        "  Args:\n",
        "      model (torch.nn.Module): The segmentation model to evaluate.\n",
        "      dataloader (torch.utils.data.DataLoader): DataLoader providing batches of data.\n",
        "      num_classes (int): Number of classes in the segmentation task.\n",
        "      filename (str, optional): Filename to save results (e.g., confusion matrix).\n",
        "\n",
        "  Returns:\n",
        "      float: Overall accuracy of the model.\n",
        "      torch.Tensor (optional): Confusion matrix (if filename is provided).\n",
        "  \"\"\"\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    evaluator = Evaluator(num_classes)\n",
        "\n",
        "\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        evaluator.update(targets, outputs)\n",
        "\n",
        "    overall_metrics = evaluator.compute()\n",
        "\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(\"Overall Evaluation Metrics:\\n\")\n",
        "        for metric, value in overall_metrics.items():\n",
        "            f.write(f\"{metric}: {value}\\n\")\n",
        "\n",
        "    return overall_metrics\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9cBUpEFjawcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNYmeMOg_kGL"
      },
      "source": [
        "### Evaluate model 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "W4WK7HeazNVv"
      },
      "outputs": [],
      "source": [
        "# Demonstrate evaluation of model 1\n",
        "def evaluate_model(model, loss_function, eval_data, num_classes,\n",
        "                   filename, gpu=True):\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "    # Move the model to the device\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    evaluator = Evaluator(num_classes)  # Create an Evaluator instance\n",
        "    with torch.no_grad():\n",
        "        for img_chips, labels in eval_data:\n",
        "            if gpu:\n",
        "                img_chips = img_chips.cuda()\n",
        "                labels = labels.cuda()\n",
        "            # Forward pass\n",
        "            outputs = model(img_chips)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            evaluator.add_batch(labels.cpu().numpy(), predicted.cpu().numpy())\n",
        "    # Calculate metrics\n",
        "    pixel_accuracy = evaluator.Pixel_Accuracy()\n",
        "    pixel_accuracy_class = evaluator.Pixel_Accuracy_Class()\n",
        "    mean_iou = evaluator.Mean_Intersection_over_Union()\n",
        "    frequency_weighted_iou = evaluator.Frequency_Weighted_Intersection_over_Union()\n",
        "\n",
        "    # Print metrics reults\n",
        "    print(f\"Model 1 {loss_function.__class__.__name__}:\")\n",
        "    print(f\"Model 1 Pixel Accuracy: {pixel_accuracy:.4f}\")\n",
        "    print(f\"Model 1 Pixel Accuracy Class: {pixel_accuracy_class:.4f}\")\n",
        "    print(f\"Model 1 Mean IoU: {mean_iou:.4f}\")\n",
        "    print(f\"Frequency Weighted IoU: {frequency_weighted_iou:.4f}\")\n",
        "\n",
        "    metrics = {\n",
        "        \"Pixel Accuracy\": pixel_accuracy,\n",
        "        \"Pixel Accuracy Class\": pixel_accuracy_class,\n",
        "        \"Mean Iou\": mean_iou,\n",
        "        \"Frequency Weighted Iou\": frequency_weighted_iou\n",
        "    }\n",
        "\n",
        "    report = pd.DataFrame([metrics])\n",
        "    report.to_csv(filename, index=False)\n",
        "    print(f\"Metrics saved to {filename}\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = BalancedCrossEntropyLoss()\n",
        "\n",
        "\n",
        "evaluate_model(model, loss_function, val_loader, num_classes=3,\n",
        "               filename='model1_evaluation.csv', gpu=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcBTKiP5f0iu",
        "outputId": "3d507223-9a6e-4d15-f5fc-f92afef271ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1 BalancedCrossEntropyLoss:\n",
            "Model 1 Pixel Accuracy: 0.8680\n",
            "Model 1 Pixel Accuracy Class: 0.7112\n",
            "Model 1 Mean IoU: 0.6301\n",
            "Frequency Weighted IoU: 0.7651\n",
            "Metrics saved to model1_evaluation.csv\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-e1cd57e727c7>:11: RuntimeWarning: invalid value encountered in divide\n",
            "  Acc = np.diag(self.confusion_matrix) / self.confusion_matrix.sum(axis=1)\n",
            "<ipython-input-28-e1cd57e727c7>:16: RuntimeWarning: invalid value encountered in divide\n",
            "  MIoU = np.diag(self.confusion_matrix) / (\n",
            "<ipython-input-28-e1cd57e727c7>:26: RuntimeWarning: invalid value encountered in divide\n",
            "  iu = np.diag(self.confusion_matrix) / (\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4ioesk4_kGL"
      },
      "source": [
        "### Evaluate model 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7wKsdVoXzWdY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e8559df-1f16-4b6c-a46e-cb9d8c5982b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1 BalancedCrossEntropyLoss:\n",
            "Model 1 Pixel Accuracy: 0.8680\n",
            "Model 1 Pixel Accuracy Class: 0.7112\n",
            "Model 1 Mean IoU: 0.6301\n",
            "Frequency Weighted IoU: 0.7651\n",
            "Metrics saved to model2_evaluation.csv\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-e1cd57e727c7>:11: RuntimeWarning: invalid value encountered in divide\n",
            "  Acc = np.diag(self.confusion_matrix) / self.confusion_matrix.sum(axis=1)\n",
            "<ipython-input-28-e1cd57e727c7>:16: RuntimeWarning: invalid value encountered in divide\n",
            "  MIoU = np.diag(self.confusion_matrix) / (\n",
            "<ipython-input-28-e1cd57e727c7>:26: RuntimeWarning: invalid value encountered in divide\n",
            "  iu = np.diag(self.confusion_matrix) / (\n"
          ]
        }
      ],
      "source": [
        "# Demonstrate evaluation of model 2\n",
        "loss_function = BalancedCrossEntropyLoss()\n",
        "\n",
        "\n",
        "evaluate_model(model, loss_function, val_loader, num_classes=3,\n",
        "               filename='model2_evaluation.csv', gpu=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font color='red'> 9/10 points on CA3</font>**\n",
        "\n",
        "**<font color='blue'> you did the evaluation for the same model, you needed to pass model to 2 versions to avoid this.</font>**"
      ],
      "metadata": {
        "id": "m1865pgrLw-s"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6tINf0Xo0vI"
      },
      "source": [
        "## OPTIONAL: Visualizing activation maps and learned kernels\n",
        "\n",
        "Using code that extracts these from intermediate layers in the network\n",
        "\n",
        "(5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wzg50ZPP_kGL"
      },
      "source": [
        "### Load a multispectral satellite image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LS1rKkW3pEdi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        },
        "outputId": "6286e854-6669-4fbc-e999-7124f3a5e107"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RasterioIOError",
          "evalue": "path/to/MSI.tif: No such file or directory",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32mrasterio/_base.pyx\u001b[0m in \u001b[0;36mrasterio._base.DatasetBase.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mrasterio/_base.pyx\u001b[0m in \u001b[0;36mrasterio._base.open_dataset\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mrasterio/_err.pyx\u001b[0m in \u001b[0;36mrasterio._err.exc_wrap_pointer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mCPLE_OpenFailedError\u001b[0m: path/to/MSI.tif: No such file or directory",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRasterioIOError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-4796a4ea6b37>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mrio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path/to/MSI.tif\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Read the image data as a numpy array and reshape to HWC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Normalize the image data to be between 0 and 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/rasterio/env.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0menv_ctor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/rasterio/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, width, height, count, crs, transform, dtype, nodata, sharing, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msharing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"r+\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             dataset = get_writer_for_path(path, driver=driver)(\n",
            "\u001b[0;32mrasterio/_base.pyx\u001b[0m in \u001b[0;36mrasterio._base.DatasetBase.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mRasterioIOError\u001b[0m: path/to/MSI.tif: No such file or directory"
          ]
        }
      ],
      "source": [
        "with rio.open(\"path/to/MSI.tif\") as dataset:\n",
        "    # Read the image data as a numpy array and reshape to HWC\n",
        "    image = dataset.read().transpose([1, 2, 0])\n",
        "\n",
        "# Normalize the image data to be between 0 and 1\n",
        "image = do_normalization(image)\n",
        "\n",
        "# Convert the image to a PyTorch tensor and add a batch dimension\n",
        "image_tensor = torch.from_numpy(image).float().permute(2, 0, 1).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMPmi3hi_kGL"
      },
      "source": [
        "### Define your CNN model and load the pre-trained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4H0lHuj7sRr"
      },
      "outputs": [],
      "source": [
        "model = unet_sus()\n",
        "model.load_state_dict(torch.load('path/to/traind_model_params.pth'))\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaE5oUzc7wST"
      },
      "outputs": [],
      "source": [
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "    return hook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9KxtDGJ_kGL"
      },
      "source": [
        "### Visualize the activation map for a particular layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HN8GqQIG7z5E"
      },
      "outputs": [],
      "source": [
        "layer_index = 2  # Choose the index of the layer to visualize (0-indexed)\n",
        "layer_name = f'conv{layer_index + 1}'  # Layer name for displaying in plot title\n",
        "activation = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7y5mHSDm78xQ"
      },
      "outputs": [],
      "source": [
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "model.conv1.register_forward_hook(get_activation(layer_name))\n",
        "\n",
        "with torch.no_grad():\n",
        "    model(image_tensor)\n",
        "\n",
        "activation = activation.squeeze().numpy()\n",
        "\n",
        "plt.imshow(activation[layer_index])\n",
        "plt.title(f'Activation Map for {layer_name}')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}